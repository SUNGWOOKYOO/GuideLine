{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python is /home/kddlab/anaconda3/envs/swyoo_dl/bin/python\n",
      "3.6.9\n",
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "!type python \n",
    "from tqdm import tqdm\n",
    "from platform import python_version\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "print(python_version())\n",
    "print(tf.__version__)\n",
    "# use GPU\n",
    "conf = tf.ConfigProto()\n",
    "conf.gpu_options.allow_growth = True\n",
    "# print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape : (55000, 28, 28)\n",
      "valid_x.shape : (5000, 28, 28)\n",
      "test_x.shape : (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# using tensorflow_gpu=1.12.0, dataset can be downloaded\n",
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "train_x = train_x.astype(np.float32).reshape(-1, T*D) / 255.0  # (784,)\n",
    "test_x = test_x.astype(np.float32).reshape(-1, T*D) / 255.0\n",
    "train_y = train_y.astype(np.int32)\n",
    "test_y = test_y.astype(np.int32)\n",
    "valid_x, train_x = train_x[:5000], train_x[5000:]\n",
    "valid_y, train_y = train_y[:5000], train_y[5000:]\n",
    "train_x = train_x.reshape([-1, T, D])\n",
    "test_x = test_x.reshape([-1, T, D])\n",
    "valid_x = valid_x.reshape([-1, T, D])\n",
    "\n",
    "print('train_x.shape :', train_x.shape)\n",
    "print('valid_x.shape :', valid_x.shape)\n",
    "print('test_x.shape :', test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" helper functions \"\"\"\n",
    "# yield mini-batch function\n",
    "def get_minibatch(X, Y, b):\n",
    "    \"\"\"\n",
    "    inputs: \n",
    "    - X: data features, shape (N, D)\n",
    "    - Y: labels, shape (N, )\n",
    "    - b: batch size, shape ()\n",
    "    \n",
    "    outputs:\n",
    "    - x_mini, y_mini: minibatch dataset\n",
    "    \"\"\"\n",
    "    step = len(X) // b\n",
    "    # yield minibatch for each step\n",
    "    for indices in np.array_split(np.random.permutation(len(X)), step):\n",
    "        x_mini, y_mini = X[indices], Y[indices]\n",
    "        yield x_mini, y_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" layers module\"\"\"\n",
    "class Model:\n",
    "    # computation graph \n",
    "    def __init__(self, T=28, D=28, H=150, DH=128, K=10, L=3, beta=0.01):\n",
    "        \"\"\"\n",
    "        hyper parameters\n",
    "        - T: number of pixel @vertical line in a image \n",
    "        - D: number of pixel @horizontal line in a image\n",
    "        - H: considering information @horizentoal line and vertical line, transform 28 size to 150 size latent vector\n",
    "        - DH: unit size of dense layer \n",
    "        - K: probabilties that logits for 10 class\n",
    "        - L: number of RNN layers \n",
    "        Model API\n",
    "        __init__(self, T=28, D=28, H=150, DH=128, K=10, L=3, beta=0.01):\n",
    "        \"\"\"\n",
    "        # feed\n",
    "        self.X = tf.placeholder(dtype=tf.float32, shape=[None, T, D])\n",
    "        self.Y = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "        self.dropout = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # reg룰 사용하면, L2 loss term들이 list로 모이고, tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)를 통해 얻는다.\n",
    "        reg = tf.contrib.layers.l2_regularizer(scale=beta)\n",
    "        init = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "#         with tf.variable_scope(\"RNN_layer\", regularizer=reg, initializer=init):\n",
    "#             # outputs: hidden features for each time step \n",
    "#             # state: hidden feature for last time step, that is, outputs[-1]\n",
    "#             # state는 이미지한장에 대해 encoding된 정보가 들어있다.\n",
    "#             cell = tf.nn.rnn_cell.BasicRNNCell(num_units=H)\n",
    "#             outputs, states = tf.nn.dynamic_rnn(cell, self.X, dtype=tf.float32)\n",
    "    \n",
    "#         with tf.variable_scope(\"Dense_layer\", regularizer=reg, initializer=init):\n",
    "#             # dense layer \n",
    "#             logits = tf.layers.dense(states, K)\n",
    "        \n",
    "        with tf.variable_scope(\"RNN_layer\", regularizer=reg, initializer=init):\n",
    "            # outputs: hidden features for each time step \n",
    "            # states: tuple information ((cell state, hidden state), ..) layer갯수 만큼의 tuple\n",
    "            # state는 이미지한장에 대해 encoding된 정보가 들어있다.\n",
    "            cells = [tf.nn.rnn_cell.LSTMCell(num_units=H) for layer in range(L)]\n",
    "            block = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "            outputs, states = tf.nn.dynamic_rnn(block, self.X, dtype=tf.float32)\n",
    "            \n",
    "        with tf.variable_scope(\"Dense_layer\", regularizer=reg, initializer=init):\n",
    "            # dense layer \n",
    "            # input은 block에서 가장 마지막 layer의 마지막 시간의 hidden state값으로 한다.(포괄적인 encoding된 정보) \n",
    "            d1 = tf.layers.dense(states[-1][-1], units=DH)\n",
    "            d1 = tf.nn.dropout(d1, keep_prob=self.dropout)\n",
    "            logits = tf.layers.dense(d1, units=K)\n",
    "        \n",
    "        with tf.variable_scope(\"loss_layer\"):\n",
    "            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.Y, logits=logits))\n",
    "            self.loss += tf.reduce_sum(reg_losses)  # scalar\n",
    "        \n",
    "        # evaluation metric\n",
    "        self.pred = tf.argmax(logits, axis=1)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.pred, self.Y), dtype=tf.float32))\n",
    "        \n",
    "    # train and evaluation  \n",
    "    def fit(self, config, train_x, train_y, valid_x, valid_y, epoch, lr, b, dr, keep_prob=0.7, \n",
    "            save=False, SAVE_FILE='./models_mnist/model', log=False, LOG_DIR='./models_mnist'):\n",
    "        \"\"\"\n",
    "        hyper parameters\n",
    "        - epoch = 10\n",
    "        - lr = 0.001\n",
    "        - b = 150 # minibatch size\n",
    "        - dr = 0.97 # learning rate decay rate\n",
    "        - keep_prob = 0.7\n",
    "        \"\"\"\n",
    "        step = tf.get_variable(name=\"global_step\", shape=(), initializer=tf.zeros_initializer(), trainable=False)\n",
    "        # each decay_step, learning rate will be decreased by decay_rate\n",
    "        # decayed_learning_rate = learning_rate * decay_rate^(global_step / decay_steps) \n",
    "        lr = tf.train.exponential_decay(learning_rate=lr, global_step=step, decay_steps=100, decay_rate=dr, staircase=True)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(self.loss, global_step=step)\n",
    "        \n",
    "        _, H, W = train_x.shape\n",
    "        \n",
    "        # save options \n",
    "        saver = tf.train.Saver(var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n",
    "        # saver = tf.train.Saver()\n",
    "        \n",
    "        # tensorboard visualization\n",
    "        if log:\n",
    "            train_loss = tf.get_variable(name='train_loss', shape=(), dtype=tf.float32, initializer=tf.zeros_initializer(), trainable=False)\n",
    "            valid_acc = tf.get_variable(name='valid_acc', shape=(), dtype=tf.float32, initializer=tf.zeros_initializer(), trainable=False)\n",
    "            train_loss_summ = tf.summary.scalar('train_loss', train_loss)\n",
    "            valid_acc_summ = tf.summary.scalar('valid_acc', valid_acc)\n",
    "            summ_op = tf.summary.merge([train_loss_summ, valid_acc_summ])\n",
    "        \n",
    "        with tf.Session(config=config) as sess:\n",
    "            # initialize all variables \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # add graph to tensorboard\n",
    "            if log:\n",
    "                print(\"@terminal: $ tensorboard --logdir={}\".format(LOG_DIR))\n",
    "                writer = tf.summary.FileWriter(LOG_DIR, session=sess)\n",
    "                writer.add_graph(sess.graph)\n",
    "            # minibatch training \n",
    "            k = 0\n",
    "            for e in range(epoch):\n",
    "                loss_train = 0\n",
    "                for i, xy in tqdm(enumerate(get_minibatch(train_x, train_y, b)), desc='Train'):\n",
    "                    # minibatch dataset\n",
    "                    x, y = xy\n",
    "                    feed = {self.X: x, self.Y: y, self.dropout: keep_prob}\n",
    "                    loss_mini, _ = sess.run([self.loss, optimizer], feed_dict=feed)\n",
    "                    loss_train += loss_mini\n",
    "                loss_train = loss_train / (i+1)\n",
    "                \n",
    "                # evaluation\n",
    "                acc_valid = self.accuracy.eval(feed_dict={self.X: valid_x, self.Y: valid_y, self.dropout: 1})\n",
    "                print('epoch {} \\t| loss: {:.4f} \\t| acc_valid: {:.4f} \\t| lr: {:0.5} '.format(e+1, loss_train, acc_valid, lr.eval()))\n",
    "                if log:\n",
    "                    sess.run([train_loss.assign(loss_train), valid_acc.assign(acc_valid)])\n",
    "                    summary = sess.run(summ_op)\n",
    "                    writer.add_summary(summary, global_step=k)\n",
    "                    k += 1\n",
    "            print(\"Training End\")\n",
    "            if save: \n",
    "                saver.save(sess, SAVE_FILE)\n",
    "                print(\"save model @{}\".format(SAVE_FILE))\n",
    "    # restore model and evaluation\n",
    "    def test(self, config, test_x, test_y, SAVE_FILE='./models_mnist/model'):\n",
    "        \"\"\"\n",
    "        restore trained weights and evaluate a model\n",
    "        \"\"\"\n",
    "        saver = tf.train.Saver(var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n",
    "        # saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session(config=config) as sess:\n",
    "            saver.restore(sess, SAVE_FILE)\n",
    "            print(\"restore is completed\")\n",
    "            acc_test = self.accuracy.eval(feed_dict={self.X: test_x, self.Y: test_y, self.dropout: 1})\n",
    "            print(\"acc_test: {}\".format(acc_test))\n",
    "            \n",
    "    def summary(self):\n",
    "        # print(\"=============================================\")\n",
    "        # print(\"list of all parameters\")\n",
    "        # print(\"=============================================\")\n",
    "        # for x in tf.global_variables():\n",
    "        #     print(x)\n",
    "        \n",
    "        print(\"=============================================\")\n",
    "        print(\"list of all trainable parameters\")\n",
    "        print(\"=============================================\")\n",
    "        for x in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "            print(x)\n",
    "        \n",
    "        # print(\"=============================================\")\n",
    "        # print(\"list of parameters reflected regularization \")\n",
    "        # print(\"=============================================\")\n",
    "        # for x in tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES):\n",
    "        #     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@terminal: $ tensorboard --logdir=./models_mnist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 366it [00:08, 45.34it/s]\n",
      "Train: 5it [00:00, 42.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 \t| loss: 1.9373 \t| acc_valid: 0.8676 \t| lr: 0.00091267 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 366it [00:07, 47.22it/s]\n",
      "Train: 5it [00:00, 46.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 \t| loss: 0.9158 \t| acc_valid: 0.9114 \t| lr: 0.00080798 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 366it [00:07, 47.15it/s]\n",
      "Train: 5it [00:00, 47.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 \t| loss: 0.7787 \t| acc_valid: 0.9086 \t| lr: 0.00073742 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 366it [00:07, 47.21it/s]\n",
      "Train: 5it [00:00, 47.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 \t| loss: 0.7054 \t| acc_valid: 0.9364 \t| lr: 0.00065284 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 366it [00:07, 47.32it/s]\n",
      "Train: 5it [00:00, 47.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 \t| loss: 0.6598 \t| acc_valid: 0.9420 \t| lr: 0.00057795 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 366it [00:07, 47.29it/s]\n",
      "Train: 5it [00:00, 47.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 \t| loss: 0.6225 \t| acc_valid: 0.9474 \t| lr: 0.00052748 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 366it [00:07, 47.07it/s]\n",
      "Train: 5it [00:00, 46.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 \t| loss: 0.5948 \t| acc_valid: 0.9488 \t| lr: 0.00046698 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 366it [00:07, 47.22it/s]\n",
      "Train: 5it [00:00, 47.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 \t| loss: 0.5789 \t| acc_valid: 0.9496 \t| lr: 0.00041341 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 366it [00:07, 47.41it/s]\n",
      "Train: 5it [00:00, 44.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 \t| loss: 0.5616 \t| acc_valid: 0.9492 \t| lr: 0.00037731 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 366it [00:07, 47.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 \t| loss: 0.5487 \t| acc_valid: 0.9586 \t| lr: 0.00033403 \n",
      "Training End\n",
      "save model @./models_mnist/model\n",
      "=============================================\n",
      "list of all trainable parameters\n",
      "=============================================\n",
      "<tf.Variable 'RNN_layer/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(178, 600) dtype=float32_ref>\n",
      "<tf.Variable 'RNN_layer/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0' shape=(600,) dtype=float32_ref>\n",
      "<tf.Variable 'RNN_layer/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0' shape=(300, 600) dtype=float32_ref>\n",
      "<tf.Variable 'RNN_layer/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0' shape=(600,) dtype=float32_ref>\n",
      "<tf.Variable 'RNN_layer/rnn/multi_rnn_cell/cell_2/lstm_cell/kernel:0' shape=(300, 600) dtype=float32_ref>\n",
      "<tf.Variable 'RNN_layer/rnn/multi_rnn_cell/cell_2/lstm_cell/bias:0' shape=(600,) dtype=float32_ref>\n",
      "<tf.Variable 'Dense_layer/dense/kernel:0' shape=(150, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Dense_layer/dense/bias:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Dense_layer/dense_1/kernel:0' shape=(128, 10) dtype=float32_ref>\n",
      "<tf.Variable 'Dense_layer/dense_1/bias:0' shape=(10,) dtype=float32_ref>\n",
      "INFO:tensorflow:Restoring parameters from ./models_mnist/model\n",
      "restore is completed\n",
      "acc_test: 0.953499972820282\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "_, T, D = train_x.shape\n",
    "H, K, L = 150, 10, 3\n",
    "model = Model(T=T, D=D, H=H, K=K, L=L, beta=0.01)\n",
    "model.fit(conf, train_x, train_y, valid_x, valid_y, epoch=10, lr=0.001, b=150, dr=0.97, keep_prob=0.7, save=True, log=True)\n",
    "model.summary()\n",
    "model.test(conf, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swyoo_dl",
   "language": "python",
   "name": "swyoo_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
